# AUTOGENERATED! DO NOT EDIT! File to edit: 00_unet.ipynb (unless otherwise specified).

__all__ = ['UnetBlockDown', 'UnetBlockUp', 'UnetBlockDownBatchNorm', 'UnetBlockUpBatchNorm', 'Unet', 'UnetBatchNorm']

# Cell
import torch
from .helpers import *
import torch.nn as nn

# Cell
#hide
class UnetBlockDown(nn.Module):

    def __init__(self, in_channel, out_channel):
        super().__init__()
        self.stack = nn.Sequential(
            nn.Conv2d(in_channel, out_channel, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(out_channel, out_channel, 3, padding=1),
            nn.ReLU(),
        )
    def forward(self, x):
        return self.stack(x)

class UnetBlockUp(nn.Module):

    def __init__(self, in_channel, out_channel):
        super().__init__()

        self.stack = nn.Sequential(
            nn.ConvTranspose2d(in_channel, in_channel, 3, padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(in_channel, out_channel, 3, padding=1),
            nn.ReLU()
        )
    def forward(self, x):
        return self.stack(x)

# Cell
#hide
class UnetBlockDownBatchNorm(nn.Module):

    def __init__(self, in_channel, out_channel):
        super().__init__()
        self.stack = nn.Sequential(
            nn.Conv2d(in_channel, out_channel, 3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(out_channel),
            nn.Conv2d(out_channel, out_channel, 3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(out_channel)
        )
    def forward(self, x):
        return self.stack(x)

class UnetBlockUpBatchNorm(nn.Module):

    def __init__(self, in_channel, out_channel):
        super().__init__()

        self.stack = nn.Sequential(
            nn.ConvTranspose2d(in_channel, in_channel, 3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(in_channel),
            nn.ConvTranspose2d(in_channel, out_channel, 3, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(out_channel)
        )
    def forward(self, x):
        return self.stack(x)

# Cell
#hide
class Unet(nn.Module):

    def __init__(self):
        super().__init__()
        self.block_down_1 = UnetBlockDown(3,64)
        self.block_down_2 = UnetBlockDown(64,128)

        self.pool = nn.MaxPool2d(2, stride=2)
        self.up = nn.Upsample(scale_factor=2, mode='bilinear')

        self.bottom = nn.Sequential(
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU()
        )

        self.block_up_1 = UnetBlockUp(128,64)
        self.block_up_2 = UnetBlockUp(64,32)
        self.output_layer = nn.Conv2d(32, 3, 1)

    def forward(self, x):
        x_1 = self.block_down_1(x)
        x= self.pool(x_1)

        x_2 = self.block_down_2(x)
        x = self.pool(x_2)

        x = self.bottom(x)
        x = self.up(x)+x_2
        x = self.block_up_1(x)

        x = self.up(x)+x_1
        x = self.block_up_2(x)
        x = self.output_layer(x)

        return x

# Cell
#hide
class UnetBatchNorm(nn.Module):

    def __init__(self):
        super().__init__()
        self.block_down_1 = UnetBlockDownBatchNorm(3,64)
        self.block_down_2 = UnetBlockDownBatchNorm(64,128)

        self.pool = nn.MaxPool2d(2, stride=2)
        self.up = nn.Upsample(scale_factor=2, mode='bilinear')

        self.bottom = nn.Sequential(
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU()
        )

        self.block_up_1 = UnetBlockUpBatchNorm(128,64)
        self.block_up_2 = UnetBlockUpBatchNorm(64,32)
        self.output_layer = nn.Conv2d(32, 3, 1)

    def forward(self, x):
        x_1 = self.block_down_1(x)
        x= self.pool(x_1)

        x_2 = self.block_down_2(x)
        x = self.pool(x_2)

        x = self.bottom(x)
        x = self.up(x)+x_2
        x = self.block_up_1(x)

        x = self.up(x)+x_1
        x = self.block_up_2(x)
        x = self.output_layer(x)

        return x