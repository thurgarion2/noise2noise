# AUTOGENERATED! DO NOT EDIT! File to edit: 00_architecture_to_test.ipynb (unless otherwise specified).

__all__ = ['ResNetBlock', 'ConvNet', 'ResNet', 'Unet', 'UnetWithConcat']

# Cell
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F

# Cell
class ResNetBlock(nn.Module):

    def __init__(self, channels):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Conv2d(channels, channels, kernel_size=(3, 3), padding=(1, 1), bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels, channels, kernel_size=(3, 3), padding=(1, 1), bias=False),
            nn.BatchNorm2d(channels))

    def forward(self, x):
        return F.relu(self.layers(x)+x)


# Cell
class ConvNet(nn.Module):

    def __init__(self):
        super().__init__()
        channels = [(3,64),(64,64),(64,64),(64,128),(128,128),(128,256),(256,256),(256,3)]
        self.layers = nn.Sequential(*[nn.Sequential(
            nn.Conv2d(ch_in, ch_out, kernel_size=(3, 3), padding=(1, 1), bias=False),
            nn.BatchNorm2d(ch_out),
            nn.ReLU(inplace=True)) for ch_in, ch_out in channels
        ])
        self.last_layer = nn.Conv2d(3, 3, kernel_size=(3, 3), padding=(1, 1))

    def forward(self, x):
        x = self.layers(x)
        return torch.sigmoid(self.last_layer(x))


# Cell
class ResNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.block_64 = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=(3, 3), padding=(1, 1), bias=False),
            nn.BatchNorm2d(64),
            ResNetBlock(64)
        )

        self.block_128 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=(1, 1), bias=False),
            nn.BatchNorm2d(128),
            ResNetBlock(128),
            ResNetBlock(128)
        )


        self.block_256 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=(3, 3), padding=(1, 1), bias=False),
            nn.BatchNorm2d(256),
            ResNetBlock(256)
        )

        self.last_layer = nn.Conv2d(256, 3, kernel_size=3, padding=1)

    def forward(self, x):
        x = self.block_64(x)
        x = self.block_128(x)
        x = self.block_256(x)
        return torch.sigmoid(self.last_layer(x))

# Cell

class Unet(nn.Module):
    def __init__(self):
        super().__init__()



        channels = [(3,64), (64,128), (128,256)]
        self.encoder = nn.ModuleList([nn.Sequential(nn.Conv2d(ch_in, ch_out, kernel_size=3, padding=1),
                                                    nn.BatchNorm2d(ch_out),
                                                    ResNetBlock(ch_out)) for ch_in, ch_out in channels])
        self.down = nn.MaxPool2d(2, stride=2)
        self.decoder = self.make_decoder_from_encoder(self.encoder)




        self.middle = ResNetBlock(256)
        self.last_layer = nn.Conv2d(3, 3, kernel_size=3, padding=1)


    def make_decoder_from_encoder(self, encoder):
        decoder = []

        x = torch.zeros((2,3,32,32))

        for l in encoder:
            last_channels = x.size(1)
            x = l(x)
            channels = x.size(1)
            decoder.append(nn.Sequential(ResNetBlock(channels),
                                             nn.Conv2d(channels, last_channels, kernel_size=3, padding=1),
                                             nn.BatchNorm2d(last_channels)))

        decoder.reverse()
        return nn.ModuleList(decoder)



    def forward(self, x):
        intermediary_x = []


        for l in self.encoder:
            x = l(x)
            intermediary_x.append(x)
            x = self.down(x)


        x = self.middle(x)
        intermediary_x.reverse()

        for l, z in zip(self.decoder, intermediary_x):
            x = F.interpolate(x,z.shape[2:])
            x = l(x+z)


        return torch.sigmoid(self.last_layer(x))

# Cell
class UnetWithConcat(nn.Module):
    def __init__(self):
        super().__init__()



        channels = [(3,64), (64,128), (128,256)]
        self.encoder = nn.ModuleList([nn.Sequential(nn.Conv2d(ch_in, ch_out, kernel_size=3, padding=1),
                                                    nn.BatchNorm2d(ch_out),
                                                    ResNetBlock(ch_out)) for ch_in, ch_out in channels])
        self.down = nn.MaxPool2d(2, stride=2)
        self.decoder = self.make_decoder_from_encoder(self.encoder)




        self.middle = ResNetBlock(256)
        self.last_layer = nn.Conv2d(6, 3, kernel_size=3, padding=1)


    def make_decoder_from_encoder(self, encoder):
        decoder = []

        x = torch.zeros((2,3,32,32))

        for l in encoder:
            last_channels = x.size(1)
            x = l(x)
            channels = x.size(1)
            decoder.append(nn.Sequential(ResNetBlock(channels),
                                             nn.Conv2d(channels, last_channels, kernel_size=3, padding=1),
                                             nn.BatchNorm2d(last_channels)))

        decoder.reverse()
        return nn.ModuleList(decoder)



    def forward(self, x):
        intermediary_x = []
        img = x

        for l in self.encoder:
            x = l(x)
            intermediary_x.append(x)
            x = self.down(x)


        x = self.middle(x)
        intermediary_x.reverse()

        for l, z in zip(self.decoder, intermediary_x):
            x = F.interpolate(x,z.shape[2:])
            x = l(x+z)


        return torch.sigmoid(self.last_layer(torch.cat((x, img), 1)))